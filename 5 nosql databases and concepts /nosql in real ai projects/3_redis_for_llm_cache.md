
# âš¡ 3 â€“ Redis for LLM Caching (Speed Up Repeated Prompts)

Calling large language models (LLMs) like GPT, Claude, or Gemini can be **expensive and slow**. Redis is the perfect tool to **cache previously computed outputs**.

---

## ğŸ§© Project Scenario

You have a web app or chatbot that frequently gets **repetitive prompts** like:
> â€œExplain transformers in simple terms.â€

Instead of paying for every request, cache results using Redis.

---

## âš™ï¸ Tech Stack

| Component        | Tool              |
|------------------|-------------------|
| Cache Layer      | Redis             |
| LLM API          | OpenAI / Claude   |
| Language         | Python            |
| Library          | redis-py          |

---

## ğŸ› ï¸ 1. Setup Redis

### Install Redis locally:

```bash
brew install redis      # macOS
sudo apt install redis  # Ubuntu
```

### Install Redis client:

```bash
pip install redis openai
```

---

## ğŸ”Œ 2. Connect to Redis

```python
import redis

r = redis.Redis(host='localhost', port=6379, db=0)
```

---

## ğŸ’¡ 3. LLM Call with Redis Cache

```python
import openai

def ask_llm(prompt: str) -> str:
    # Check cache first
    cached = r.get(prompt)
    if cached:
        print("âœ… Cache hit")
        return cached.decode()

    # Else, call LLM
    print("âŒ Cache miss")
    response = openai.ChatCompletion.create(
        model="gpt-4",
        messages=[{"role": "user", "content": prompt}]
    )
    answer = response.choices[0].message.content

    # Cache it for 1 hour
    r.setex(prompt, 3600, answer)
    return answer
```

---

## ğŸ§ª Test It

```python
print(ask_llm("Explain Transformers in 1 sentence."))
print(ask_llm("Explain Transformers in 1 sentence."))  # Second call will hit cache
```

---

## ğŸ“¦ Redis Key Structure (Best Practice)

| Key Example                             | Value Type     | Purpose                      |
|-----------------------------------------|----------------|------------------------------|
| `prompt:Explain Transformers`           | string         | Store response               |
| `chat:user123:session456`               | JSON           | Conversation memory          |
| `usage:user42:count`                    | counter        | Rate limiting / billing      |
| `model:gpt-4:config`                    | JSON/dict      | Track model config/version   |

---

## ğŸš€ Advanced: JSON Hash Cache

```python
prompt_key = "cache:gpt:prompt"
r.hset(prompt_key, prompt, answer)
r.expire(prompt_key, 3600)
```

---

## ğŸ“ Project Layout

```
redis-llm-cache/
â”œâ”€â”€ cache.py         # Redis connection logic
â”œâ”€â”€ llm_cache.py     # Main logic to check/store LLM calls
â”œâ”€â”€ test.py          # Run tests
â”œâ”€â”€ requirements.txt
```

---

## ğŸ’¼ Real-World Use Cases

| Use Case                     | Redis Feature   |
|------------------------------|-----------------|
| LLM response reuse           | String TTL keys |
| Prevent repeated prompts     | Hash keys       |
| Prompt deduplication         | Set with TTL    |
| App state tracking           | JSON / key tree |

---

## âœ… Production Tips

- Use `setex()` instead of `set()` to auto-expire cache
- Normalize prompts (strip, lowercase) before caching
- Use `redis-py` connection pool for speed
- Monitor Redis memory and TTL hit ratio

---

ğŸ“ **Next File:** [`4_streaming_user_logs_redis.md`](./4_streaming_user_logs_redis.md)
